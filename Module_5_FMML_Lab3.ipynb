{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fathimath-Rifna-VK/fmml2021/blob/main/Module_5_FMML_Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr-MnaGs7JmZ"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ob_zZms7VOu"
      },
      "source": [
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Kix4bcChiy"
      },
      "source": [
        "# Creating the Data\n",
        "\n",
        "Let's generate some data with:\n",
        "\\begin{equation} y_0= 4 \\end{equation} \n",
        "\\begin{equation} y_1= 3 \\end{equation} \n",
        "\n",
        "and also add some noise to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtAS7eFZ9hX6"
      },
      "source": [
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD95NaF-CxM-"
      },
      "source": [
        "Let's also plot the data we just created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IiEP4BQ7Wja"
      },
      "source": [
        "plt.plot(X, y, 'b.')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y', rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScwxpouoDDyZ"
      },
      "source": [
        "## Cost Function\n",
        "\n",
        "The equation for calculating cost function is as shown below. The cost function is only for linear regression. For other algorithms, the cost function will be different and the gradients would have to be derived from the cost functions\n",
        "\n",
        "\\begin{equation}\n",
        "J(y_{pred}) = 1/2m \\sum_{i=1}^{m} (h(y_{pred})^{(i)} - y^{(i)})^2 \n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUeTUAXH7ZaV"
      },
      "source": [
        "def cal_cost(y_pred, X, y):\n",
        "    '''\n",
        "    Calculates the cost for given X and Y.\n",
        "    y_pred = Vector of y_preds \n",
        "    X = Row of X's np.zeros((2, j))\n",
        "    y = Actual y's np.zeros((2, 1))\n",
        "    \n",
        "    where:\n",
        "        j is the no of features\n",
        "    '''\n",
        "    \n",
        "    m = len(y)\n",
        "    \n",
        "    predictions = X.dot(y_pred)\n",
        "    cost = (1 / 2 * m) * np.sum(np.square(predictions - y))\n",
        "\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcXqsVNpDbKC"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "\\begin{equation}\n",
        "y_{pred_0}: = y_{pred_0} -\\alpha . (1/m .\\sum_{i=1}^{m}(h(y_{pred}^{(i)} - y^{(i)}).X_0^{(i)})\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "y_{pred_1}: = y_{pred_1} -\\alpha . (1/m .\\sum_{i=1}^{m}(h(y_{pred}^{(i)} - y^{(i)}).X_0^{(i)})\n",
        "\\end{equation}\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\\begin{equation}\n",
        "y_{pred_j}: = y_{pred_j} -\\alpha . (1/m .\\sum_{i=1}^{m}(h(y_{pred}^{(i)} - y^{(i)}).X_0^{(i)})\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwxBFXP88NBW"
      },
      "source": [
        "def gradient_descent(X, y, y_pred, learning_rate=0.01, iterations=100):\n",
        "    '''\n",
        "    X = Matrix of X with added bias units\n",
        "    y = Vector of Y\n",
        "    y_pred = Vector of y_preds np.random.randn(j, 1)\n",
        "    learning_rate \n",
        "    iterations = no of iterations\n",
        "    \n",
        "    Returns the final y_pred vector and array of cost history over no of iterations\n",
        "    '''\n",
        "\n",
        "    m = len(y)\n",
        "    cost_history = np.zeros(iterations)\n",
        "    y_pred_history = np.zeros((iterations, 2))\n",
        "    \n",
        "    for it in range(iterations):    \n",
        "        prediction = np.dot(X, y_pred)\n",
        "        y_pred = y_pred - (1 / m) * learning_rate * (X.T.dot((prediction - y)))\n",
        "        y_pred_history[it,:] = y_pred.T\n",
        "        cost_history[it]  = cal_cost(y_pred, X, y)\n",
        "        \n",
        "    return y_pred, cost_history, y_pred_history    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iSohSB2EtK1"
      },
      "source": [
        "Let's do 1000 iterations with a learning rate of 0.01. \n",
        "We will start with a random prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18AX7hrU8bv5"
      },
      "source": [
        "lr = 0.01\n",
        "n_iter = 1000\n",
        "\n",
        "y_pred = np.random.randn(2,1)\n",
        "X_b = np.c_[np.ones((len(X), 1)), X]\n",
        "y_pred, cost_history, y_pred_history = gradient_descent(X_b, y, y_pred, lr, n_iter)\n",
        "\n",
        "print('y_pred[0]: {:0.3f}\\ny_pred[1]: {:0.3f}'.format(y_pred[0][0], y_pred[1][0]))\n",
        "print('Final error: {:0.3f}'.format(cost_history[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7fao2MaE216"
      },
      "source": [
        "Plotting the error vs Number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrkrAAbk8hIs"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "ax.set_ylabel('Error')\n",
        "ax.set_xlabel('Number of iterations')\n",
        "\n",
        "ax.plot(range(n_iter), cost_history, 'b.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG5tWAy-FCaW"
      },
      "source": [
        "Zooming in..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ7BoFHy8kTk"
      },
      "source": [
        "fig,ax = plt.subplots(figsize=(10,8))\n",
        "ax.plot(range(200), cost_history[:200], 'b.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYhOp3fjnh2G"
      },
      "source": [
        "# Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVwD7Cqw8m1d"
      },
      "source": [
        "def stocashtic_gradient_descent(X, y, y_pred, learning_rate=0.01, iterations=10):\n",
        "    '''\n",
        "    X = Matrix of X with added bias units\n",
        "    y = Vector of Y\n",
        "    y_pred = Vector of y_pred np.random.randn(j,1)\n",
        "    learning_rate \n",
        "    iterations = no of iterations\n",
        "    \n",
        "    Returns the final y_pred vector and array of cost history over no of iterations\n",
        "    '''\n",
        "\n",
        "    m = len(y)\n",
        "    cost_history = np.zeros(iterations)\n",
        "    \n",
        "    for it in range(iterations):\n",
        "        cost = 0.0\n",
        "        \n",
        "        for i in range(m):\n",
        "            rand_ind = np.random.randint(0,m)\n",
        "            X_i = X[rand_ind, :].reshape(1, X.shape[1])\n",
        "            y_i = y[rand_ind].reshape(1,1)\n",
        "            prediction = np.dot(X_i, y_pred)\n",
        "\n",
        "            y_pred = y_pred - (1 / m) * learning_rate *(X_i.T.dot((prediction - y_i)))\n",
        "            cost += cal_cost(y_pred, X_i, y_i)\n",
        "\n",
        "        cost_history[it]  = cost\n",
        "        \n",
        "    return y_pred, cost_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk6pfB5c8tPz"
      },
      "source": [
        "lr = 0.5\n",
        "n_iter = 50\n",
        "y_pred = np.random.randn(2, 1)\n",
        "X_b = np.c_[np.ones((len(X), 1)), X]\n",
        "y_pred, cost_history = stocashtic_gradient_descent(X_b, y, y_pred, lr, n_iter)\n",
        "\n",
        "print('y_pred[0]: {:0.3f}\\ny_pred[1]: {:0.3f}'.format(y_pred[0][0], y_pred[1][0]))\n",
        "print('Final error: {:0.3f}'.format(cost_history[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiJUgS7o8u2e"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,8))\n",
        "\n",
        "ax.set_ylabel('Error')\n",
        "ax.set_xlabel('Number of iterations')\n",
        "y_pred = np.random.randn(2,1)\n",
        "\n",
        "ax.plot(range(n_iter), cost_history, 'b.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScckWktynk1o"
      },
      "source": [
        "# Mini Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JtxFVL78wEm"
      },
      "source": [
        "def minibatch_gradient_descent(X, y, y_pred, learning_rate=0.01, iterations=10, batch_size=20):\n",
        "    '''\n",
        "    X = Matrix of X without added bias units\n",
        "    y = Vector of Y\n",
        "    y_pred = Vector of y_preds np.random.randn(j, 1)\n",
        "    learning_rate \n",
        "    iterations = no of iterations\n",
        "    \n",
        "    Returns the final theta vector and array of cost history over no of iterations\n",
        "    '''\n",
        "\n",
        "    m = len(y)\n",
        "    cost_history = np.zeros(iterations)\n",
        "    n_batches = int(m / batch_size)\n",
        "    \n",
        "    for it in range(iterations):\n",
        "        cost = 0.0\n",
        "        indices = np.random.permutation(m)\n",
        "        X = X[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "        for i in range(0, m, batch_size):\n",
        "            X_i = X[i: i + batch_size]\n",
        "            y_i = y[i: i + batch_size]\n",
        "            \n",
        "            X_i = np.c_[np.ones(len(X_i)), X_i]\n",
        "            prediction = np.dot(X_i, y_pred)\n",
        "\n",
        "            y_pred = y_pred - (1 / m) * learning_rate * (X_i.T.dot((prediction - y_i)))\n",
        "            cost += cal_cost(y_pred, X_i, y_i)\n",
        "\n",
        "        cost_history[it]  = cost\n",
        "        \n",
        "    return y_pred, cost_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpbsVwA28znL"
      },
      "source": [
        "lr = 0.1\n",
        "n_iter = 200\n",
        "y_pred = np.random.randn(2,1)\n",
        "y_pred, cost_history = minibatch_gradient_descent(X, y, y_pred, lr, n_iter)\n",
        "\n",
        "print('y_pred[0]: {:0.3f}\\ny_pred[1]: {:0.3f}'.format(y_pred[0][0], y_pred[1][0]))\n",
        "print('Final error: {:0.3f}'.format(cost_history[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_ivOYHT817C"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,8))\n",
        "\n",
        "ax.set_ylabel('Error')\n",
        "ax.set_xlabel('Number of iterations')\n",
        "y_pred = np.random.randn(2,1)\n",
        "\n",
        "ax.plot(range(n_iter), cost_history, 'b.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Sn1erIU83ck"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}