{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regression-lab4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fathimath-Rifna-VK/fmml2021/blob/main/Module_9_regression_lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zju_1EaBPpz1"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn.metrics as metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U0yQRtnVaTy"
      },
      "source": [
        "# Lab 4: Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEao5S0ki2jT"
      },
      "source": [
        "Just as we have linear and polynomial regression for predicting numerical values, we have another type of regression for predicting class labels of data, namely **logistic regression**. \n",
        "\n",
        "The reason its called logistic regression is because it makes use of the logistic function to generate output values. The logistic function is defined as:\n",
        "\n",
        "$g(z) = \\frac{1}{1 + e^{-z}}$\n",
        "\n",
        "\\\n",
        "\n",
        "The reason we use the logistic function is because its range is bounded between 0 and 1. Anything else comes to mind when talking about values between 0 and 1? Probabilities! And we saw in lab 3 how deeply related probability and classification are.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1db86qd4W_4erTMC9no5P4ylWze6c2j7V\" width=\"500\" height=\"350\" align=\"middle\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_44CGMRN_UtH"
      },
      "source": [
        "## Why use logistic function? Why not use linear regression? \n",
        "\n",
        "Let's see why we can't use linear regression for classification tasks with the help of an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWNOyCr__d0A"
      },
      "source": [
        "x = np.array([i for i in range(10,30)])\n",
        "y = np.concatenate([np.ones(10), np.zeros(10)])\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "xlabel = 'Age'\n",
        "ylabel = 'Purchased'\n",
        "plt.xlabel(xlabel)\n",
        "plt.ylabel(ylabel)\n",
        "plt.grid(color='k', linestyle=':', linewidth=1)\n",
        "_ = plt.plot(x, y, 'xb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mTwm4lDKoMV"
      },
      "source": [
        "Suppose we have some product, and we collected data on the relationship between the age of a customer, and whether they purchased the item or not. Customers aged between 10 and 19 made the purchase (purchased=1), whereas those customers above the age of 19 did not (purchased=0).\n",
        "\n",
        "\\\n",
        "\n",
        "Now, let's try and fit linear regression and logistic regression models to this data, and test it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iup50dU_oQG"
      },
      "source": [
        "def linear_regression(x,y):\n",
        "\n",
        "  lin_regression = LinearRegression()\n",
        "  lin_regression.fit(pd.DataFrame(x), y)\n",
        "\n",
        "  lin_y_pred_1 = lin_regression.predict(pd.DataFrame(x))\n",
        "\n",
        "  line_point_5 = x * 0 + .5\n",
        "\n",
        "  fig = plt.figure(figsize=(10,5))\n",
        "  xlabel = 'Age'\n",
        "  ylabel = 'Purchased'\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.grid(color='k', linestyle=':', linewidth=1)\n",
        "  plt.plot(x, y, 'xb')\n",
        "  plt.plot(x, lin_y_pred_1, '-r')\n",
        "  plt.plot(x, line_point_5,'-g')\n",
        "  plt.legend([\"Data Points\",\"Linear Regression Model\",\"Decision Boundary\"])\n",
        "  plt.show()\n",
        "\n",
        "  return lin_regression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0abeDE8aHrwq"
      },
      "source": [
        "lin_regression = linear_regression(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub5AkghRIlJk"
      },
      "source": [
        "The green line is called the decision boundary, which is basically the line that divides the two classes on the cartesian plane."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DBWBxAHHi0B"
      },
      "source": [
        "test_x = np.array([i for i in range(10,35,5)])\n",
        "test_y_pred = lin_regression.predict(pd.DataFrame(test_x))\n",
        "test_y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx6BdyEEIu8r"
      },
      "source": [
        "Since linear regression predictions are not probabilities, values greater than 0.5 are considered to belong to positive purchase class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMK9K0Ls_9sX"
      },
      "source": [
        "def logistic_regression(x,y):\n",
        "\n",
        "  log_regression = LogisticRegression()\n",
        "\n",
        "  log_regression.fit(pd.DataFrame(x), y)\n",
        "\n",
        "  y_pred = log_regression.predict_proba(pd.DataFrame(x))\n",
        "  log_y_pred_1 = [item[1] for item in y_pred]\n",
        "\n",
        "  line_point_5 = x * 0 + .5\n",
        "\n",
        "  fig = plt.figure(figsize=(10,5))\n",
        "  xlabel = 'Age'\n",
        "  ylabel = 'Purchased'\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.grid(color='k', linestyle=':', linewidth=1)\n",
        "  plt.plot(x, y, 'xb')\n",
        "  plt.plot(x, log_y_pred_1, '-r')\n",
        "  plt.plot(x, line_point_5,'-g')\n",
        "  plt.legend([\"Data Points\",\"Logistic Regression Model\",\"Decision Boundary\"])\n",
        "  plt.show()\n",
        "\n",
        "  return log_regression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB6voRBlHUSf"
      },
      "source": [
        "log_regression = logistic_regression(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Cgpr78MT-Z"
      },
      "source": [
        "**Q:** What do you observe from the two graphs? Hint: think in terms of the decision boundary....\n",
        "\n",
        "**A:** According to the linear regression model, any age greater than 19 would belong to class 0 (no purchase), and any age less than or equal to 19 would belong to class 1 (purchase made). The logistic regression curve follows the same trend. We can think of the age 19 as a turning point of sorts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBRy-dhZLQVh"
      },
      "source": [
        "Now, say we've collected more data the next day, and add the new data to the existing data we had from day 1. We find that customers aged between 60 and 70 did not make a purchase, which is aligned with our earlier data as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ME4nIUA5e1"
      },
      "source": [
        "x = np.append(x, np.array([i for i in range(60,70)]))\n",
        "y = np.append(y, np.zeros(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COwTJ5XCBQCN"
      },
      "source": [
        "lin_regression = linear_regression(x,y)\n",
        "log_regression = logistic_regression(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJKRz7FtIDnk"
      },
      "source": [
        "test_x = np.array([i for i in range(18,26)])\n",
        "\n",
        "test_y_pred_lin = lin_regression.predict(pd.DataFrame(test_x))\n",
        "print(\"Linear Regression Predictions:\")\n",
        "print(test_y_pred_lin)\n",
        "\n",
        "test_y_pred_log = log_regression.predict_proba(pd.DataFrame(test_x))[:,1]\n",
        "print(\"Logistic Regression Predictions:\")\n",
        "print(test_y_pred_log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr-07NmOKQEC"
      },
      "source": [
        "We can now see where linear regression fails. The model tries to fit as many points as it can. By doing so, the turning point which was earlier 19 has now increased to 23, which is clearly wrong for the values 20, 21 and 22, which can be seen from the predictions on the test data.\n",
        "\n",
        "\\\n",
        "\n",
        "On the other hand, the logistic regression model predicts the classes correctly, and perfectly models the relationship between the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO5SyZVNNp5Q"
      },
      "source": [
        "What we can conclude from these observations are that a probabilistic model is required for classification problems. Lab 3 covered the intuition behind MLE, cross-entropy and classification, and so now you should be able to appreaciate the motivation behind logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6iLySzwlPTm"
      },
      "source": [
        "## Applying logistic regression to MNIST digit Dataset\n",
        "\n",
        "Let's see logistic regression in action with the MNIST digits dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc5dxfJi80nC"
      },
      "source": [
        "# initializing and visualizing MNIST data\n",
        "\n",
        "digits = load_digits()\n",
        "\n",
        "plt.figure(figsize=(20,4))\n",
        "for index, (image, label) in enumerate(zip(digits.data[0:5], digits.target[0:5])):\n",
        " plt.subplot(1, 5, index + 1)\n",
        " plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
        " plt.title('Training: %i\\n' % label, fontsize = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02h6gzviOn9o"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=0)\n",
        "log_reg = LogisticRegression()\n",
        "log_reg = log_reg.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5Hpqr1HO1lT"
      },
      "source": [
        "y_pred = log_reg.predict(x_test)\n",
        "\n",
        "print(\"PREDICTIONS:\\n\")\n",
        "print(y_pred[:10])\n",
        "print(\"\\n\")\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.title(\"Test image data\")\n",
        "\n",
        "for index, (image, label) in enumerate(zip(x_test[0:10], y_test[0:10])):\n",
        " plt.subplot(2, 5, index + 1)\n",
        " plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
        " plt.title('Training: %i\\n' % label, fontsize = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IXb2AbpPzNf"
      },
      "source": [
        "# Accuracy\n",
        "\n",
        "acc = log_reg.score(x_test,y_test)\n",
        "print(\"Accuracy obtained on test set: {}\".format(round(acc*100,3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecHbLkhVQUSe"
      },
      "source": [
        "# Visualizing confusion matrix\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(9,9))\n",
        "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Reds_r');\n",
        "plt.ylabel('Actual label');\n",
        "plt.xlabel('Predicted label');\n",
        "all_sample_title = 'Accuracy Score: {0}'.format(round(acc*100,3))\n",
        "plt.title(all_sample_title, size = 15);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdAhf7JjRE37"
      },
      "source": [
        "## References\n",
        "\n",
        "1. https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a"
      ]
    }
  ]
}