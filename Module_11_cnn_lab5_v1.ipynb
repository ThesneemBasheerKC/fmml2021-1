{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_lab5-v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fathimath-Rifna-VK/fmml2021/blob/main/Module_11_cnn_lab5_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLeXDNSGioRA"
      },
      "source": [
        "# Convolutional Neural Networks - Lab 5\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        " <td>\n",
        " <a target=\"_blank\" href=\"https://colab.research.google.com/drive/1l5bgdEzYIgH8o3yTkBvneHJoGE7A2SkX?usp=sharing\"><img height=\"32px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" />Run in Google Colab</a>\n",
        " </td>\n",
        " <td>\n",
        " <a target=\"_blank\" href=\"https://github.com/Foundations-in-Modern-Machine-Learning/course-contents/blob/main/CNN/cnn_lab5.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        " </td>\n",
        "</table>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "---\n",
        "Module Coordinator: Ekta Gavas\n",
        "\n",
        "Email: ekta.gavas@research.iiit.ac.in\n",
        "\n",
        "## Description\n",
        "---\n",
        "We have used CNNs for image data. Now, one can ask: Can we use CNNs for other data modality, like text or audio? The answer is YES. CNNs can be used for non-image data as well, including text and audio. When using Naive Bayes and KNN we represent our text as a vector. With CNNs, it is similar. We pass the 1D input to our model. In case of audio, the numbers in vectors would represent an audio signal. In this lab exercise, we will see how we can use CNNs with audio data. We will perform speech command recognition by training an audio classifer network with speech command dataset. We will be using torchaudio module from Pytorch to handle audio. \n",
        "\n",
        "\n",
        "## Starter Code\n",
        "---\n",
        "To make your task easier, we provide you the starter code to perform the lab exercises. It is expected that you should try to understand what the code does and analyze the output. We will be using Pytorch framework for the implementation of this lab. The training hyperparameters that are used in the code may not be the best to minimize training time according to lab scope. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YtKOR_f5n80"
      },
      "source": [
        "## Speech Command Recognition with torchaudio\n",
        "First, letâ€™s install and import the [torchaudio](https://pytorch.org/audio/stable/index.html) package. We will also install pydub package to be used later. (for an interactive demo at the end)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKEf9aXIlYNV"
      },
      "source": [
        "!pip install torchaudio pydub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKrbBJwLmerS"
      },
      "source": [
        "# Import other required packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "from torch.utils.data import DataLoader,random_split,Dataset\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "# from pathlib import Path\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQbVbWY2mu9Y"
      },
      "source": [
        "# Device configuration (whether to run on GPU or CPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gayP_YnxmycZ"
      },
      "source": [
        "# Set seeds for reproducibility\n",
        "seed = 0\n",
        "# np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEiymjQsm1to"
      },
      "source": [
        "### Speech Commands Dataset\n",
        "This dataset consists of 105,829 one-second long utterances of\n",
        "35 words. The words were recorded by 2,618 different speakers. This audio dataset of spoken words was designed to help train and evaluate keyword spotting systems. Its primary goal is to provide a way to build and test small models that detect when a single word is spoken, from a set of target words, with as few false positives as possible from background noise or unrelated speech. Each utterance in the dataset is stored as WAVE format file. The dataset was introduced in [this paper](https://arxiv.org/abs/1804.03209).\n",
        "\n",
        "Pytorch's torchaudio module provides utilities to work with audio datasets, speech commands dataset being one of them. This is large dataset, so for reducing the dataset loading and training time, we provide a smaller version containing 10 classes of the original dataset. We will still use the 'SPEECHCOMMANDS' from the torchaudio to load the dataset.\n",
        "\n",
        "Let's download the modified dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbzbJSmdm017"
      },
      "source": [
        "# Download the dataset\n",
        "!fileid=\"14epafxPS5If_XZMQXGk_OmcgRvEVNwFZ\" && filename=\"speech_dataset.zip\" && curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\" > /dev/null && curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=${fileid}\" -o ${filename}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83gjxHUWvnbd"
      },
      "source": [
        "# Unzip the dataset archive\n",
        "!unzip -q speech_dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM7g8i0vzDTe"
      },
      "source": [
        "We will use the torchaudio dataset class to load our version of speech dataset. To do this, we will pass the path to our archive as argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eCGq1b5Av5O"
      },
      "source": [
        "train_data = torchaudio.datasets.SPEECHCOMMANDS('./' , url = 'speech_commands_v0.02', \n",
        "                                       folder_in_archive= 'speech_dataset', subset='training', download = False)\n",
        "val_data = torchaudio.datasets.SPEECHCOMMANDS('./' , url = 'speech_commands_v0.02', \n",
        "                                       folder_in_archive= 'speech_dataset', subset='validation', download = False)\n",
        "test_data = torchaudio.datasets.SPEECHCOMMANDS('./' , url = 'speech_commands_v0.02', \n",
        "                                       folder_in_archive= 'speech_dataset', subset='testing', download = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzcMN-9R5n9D"
      },
      "source": [
        "A data point in the SPEECHCOMMANDS dataset is a tuple made of a waveform\n",
        "(the audio signal), the sampling rate, the utterance (label), the ID of\n",
        "the speaker, the number of the utterance. Let's visualize the raw form of audio waveform.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PxtuYs95n9F"
      },
      "source": [
        "waveform, sample_rate, label, speaker_id, utterance_number = train_data[0]\n",
        "\n",
        "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
        "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
        "\n",
        "plt.plot(waveform.t().numpy());"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6HGUezY0YVU"
      },
      "source": [
        "Let's print the labels in our dataset. These are the speech commands said by the speakers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrbOZmXAA-9C"
      },
      "source": [
        "classes = sorted(list(set(datapoint[2] for datapoint in train_data)))\n",
        "print(classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F61LUc_60sdm"
      },
      "source": [
        "Let's listen to a few samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BgdSX9uzley"
      },
      "source": [
        "waveform, sample_rate, label, *_ = train_data[1]\n",
        "print(label)\n",
        "ipd.Audio(waveform.numpy(), rate=sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5J-ZaRN1J5G"
      },
      "source": [
        "waveform, sample_rate, label, *_ = train_data[-1]\n",
        "print(label)\n",
        "ipd.Audio(waveform.numpy(), rate=sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irPBTmZI1m-3"
      },
      "source": [
        "For the sake of simplicity, we are not applying transformations to the dataset and we will use it in its raw form. In practical scenarios, we may need to apply transforms. For example, we can downsample the audio for faster processing without losing too much of the classification power. It is common for some datasets though to have to reduce the number of channels (say from stereo to mono) by either taking the mean along the channel dimension, or simply keeping only one of the channels. Since SpeechCommands uses a single channel for audio, this is not needed here. You can look at the transforms you can apply in the references section at the end.\n",
        "\n",
        "We will drop the waveform samples in training and testing sets that don't have length as 16000. We do this to ensure same length of all samples. There are other ways to deal with this (e.g. resampling to fixed shape) but for now, we will just drop those samples. (Number of dropped samples is not very large)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYnsMBRa9_Dq"
      },
      "source": [
        "# This function will retain the samples in data with fixed length of l\n",
        "def retain_samples(data, l=16000):\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in range(0,len(data)):\n",
        "      if data[i][0].shape == torch.Size([1, l]):\n",
        "          x.append(data[i][0])\n",
        "          y.append(data[i][2])\n",
        "  print('Total samples: {} Retained samples: {}'.format(len(data), len(x)))\n",
        "  return x, y\n",
        "\n",
        "print('Training data---')\n",
        "trainx, trainy = retain_samples(train_data)\n",
        "print('Validation data---')\n",
        "valx, valy = retain_samples(val_data)\n",
        "print('Testing data---')\n",
        "testx, testy = retain_samples(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkbzuyEx_GaL"
      },
      "source": [
        "Let's create a custom dataset class to load the above waveforms data after discarding the unequal length samples with labels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU18JMmd_QSy"
      },
      "source": [
        "class SpeechDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, data, labels, list_dir,transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.label_dict = list_dir\n",
        "        self.transform = transform\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)    \n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        waveform = self.data[idx]\n",
        "        \n",
        "        if self.transform != None:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        if self.labels[idx] in self.label_dict:\n",
        "            out_labels = self.label_dict.index(self.labels[idx])\n",
        "            \n",
        "\n",
        "        return waveform, out_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQbDDHInDuUP"
      },
      "source": [
        "train_dataset= SpeechDataset(trainx, trainy, classes)\n",
        "val_dataset= SpeechDataset(valx, valy, classes)\n",
        "test_dataset= SpeechDataset(valx, valy, classes)\n",
        "\n",
        "# Data loaders\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=False)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niFouaVqE5Jt"
      },
      "source": [
        "### Define model\n",
        "As the data is one-dimensional, we will use 1D convolutions. Here, we will also explore the use of 'Dropout' layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjLWVQAYx2vr"
      },
      "source": [
        "#### Dropout\n",
        "Dropout is a regularization method where during training, some layer outputs are randomly ignored or \"dropped out.\" This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections. Here, a new hyperparameter is introduced that specifies the probability at which outputs of the layer are dropped out. It randomly zeroes some of the elements of the input to the layer with probability p using samples from a Bernoulli distribution. This has proven to be an effective technique for regularization. When using dropout regularization, it is possible to use larger networks with less risk of overfitting. We will use [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) from Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ORzuqjQE7XO"
      },
      "source": [
        "class AudioClassifier(nn.Module):\n",
        "    def __init__(self, num_class):\n",
        "        super(AudioClassifier,self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv1d(in_channels=1,out_channels=8,kernel_size=13,stride=1)\n",
        "        self.dropout1 = nn.Dropout(0.3) \n",
        "    \n",
        "        self.conv2 = nn.Conv1d(in_channels=8,out_channels=16,kernel_size=11,stride=1)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        \n",
        "        self.conv3 = nn.Conv1d(in_channels=16,out_channels=32,kernel_size=9,stride=1)\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        \n",
        "        self.conv4 = nn.Conv1d(in_channels=32,out_channels=64,kernel_size=7,stride=1)\n",
        "        self.dropout4 = nn.Dropout(0.3)\n",
        "        \n",
        "        self.fc1 = nn.Linear(12416, 256)\n",
        "        self.dropout5 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(256,128)\n",
        "        self.dropout6 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(128, num_class)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = F.max_pool1d(F.relu(self.conv1(x)),kernel_size=3)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x = F.max_pool1d(F.relu(self.conv2(x)),kernel_size=3)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x = F.max_pool1d(F.relu(self.conv3(x)),kernel_size=3)\n",
        "        x = self.dropout3(x)\n",
        "        \n",
        "        x = F.max_pool1d(F.relu(self.conv4(x)),kernel_size=3)\n",
        "        x = self.dropout4(x)\n",
        "        \n",
        "        x = F.relu(self.fc1(x.reshape(-1,x.shape[1] * x.shape[2])))\n",
        "        x = self.dropout5(x)\n",
        "        \n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout6(x)\n",
        "        \n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkswlVktFYLv"
      },
      "source": [
        "Define training and testing helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQISxcktFb1a"
      },
      "source": [
        "def train(model,trainloader,optim,scheduler,criterion,epoch):\n",
        "    print(\"Training\")\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    total = 0\n",
        "    total_correct = 0\n",
        "    \n",
        "    for inputs, targets in tqdm(trainloader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        optim.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs,targets)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        _,predicted = torch.max(outputs.data,1)\n",
        "        total_correct += (predicted == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "    \n",
        "    acc = 100. * total_correct / total\n",
        "    print(\"Epoch: {}  Loss: {:.2f} Accuracy {:.2f} \".format(epoch+1,train_loss/len(trainloader), acc))\n",
        "    return train_loss/len(trainloader), acc\n",
        "    \n",
        "def test(model,testloader,optim,criterion,epoch):\n",
        "    print(\"validation\")\n",
        "    model.eval()\n",
        "    test_loss,total,total_correct = 0,0,0\n",
        "    \n",
        "    for inputs, targets in tqdm(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        total_correct += (predicted == targets).sum().item()\n",
        "\n",
        "    acc = 100. * total_correct / total\n",
        "    print(\"\\nEpoch #{} Loss: {:.4f} Accuracy: {:.2f}\".format(epoch+1, test_loss/len(testloader), acc))\n",
        "    return test_loss/len(testloader), acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTHlK-QvE_s1"
      },
      "source": [
        "model = AudioClassifier(num_class=10)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PywwWZrBFH0D"
      },
      "source": [
        "### Define training parameters\n",
        "We will use [Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html?highlight=adam#torch.optim.Adam). In normal SGD the learning rate has an equivalent type of effect for all the parameters of the model. Whereas, Adam adaptively selects a separate learning rate for each parameter. For more information on this optimizer, check out the resources section.\n",
        "\n",
        "Here, we are using [OneCycleLR scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html). It sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZF3R4p2FMI-"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
        "                                              steps_per_epoch=int(len(trainloader)),\n",
        "                                              epochs=20,\n",
        "                                              anneal_strategy='linear') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QakSt_DiGDWC"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjv9it-tGFux"
      },
      "source": [
        "num_epochs=20\n",
        "train_losses, train_acc = [], []\n",
        "val_losses, val_acc = [], []\n",
        "\n",
        "for epoch in range(0, num_epochs):\n",
        "    loss, acc = train(model,trainloader,optimizer,scheduler,criterion,epoch)\n",
        "    train_losses.append(loss)\n",
        "    train_acc.append(acc)\n",
        "\n",
        "    loss, acc = test(model,valloader,optimizer,criterion,epoch)\n",
        "    val_losses.append(loss)\n",
        "    val_acc.append(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTv8qIlCVnMf"
      },
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(train_losses)+1),train_losses)\n",
        "ax.plot(np.arange(1,len(val_losses)+1),val_losses)\n",
        "plt.xlabel('Loss')\n",
        "plt.ylabel('Epochs')\n",
        "plt.legend(['Train Loss', 'Val Loss'])\n",
        "\n",
        "\n",
        "ax = fig.add_subplot(1,2, 2)\n",
        "ax.plot(np.arange(1,len(train_acc)+1),train_acc)\n",
        "ax.plot(np.arange(1,len(val_acc)+1),val_acc)\n",
        "plt.xlabel('Accuracy')\n",
        "plt.ylabel('Epochs')\n",
        "plt.legend(['Train Acc', 'Val Acc'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgU3pcZ3VSfd"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POlLz_CuV-Wq"
      },
      "source": [
        "loss, acc = test(model,testloader,optimizer,criterion,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMOi-1gz5n9f"
      },
      "source": [
        "Now, try with your own recordings of speaking a word from the labels!\n",
        "For example, say 'backward' while executing the cell below. The below cell will record one second of audio and load the sample in the output cell for you to hear it. We can also easily load a single audio file using torchaudio.load() method. It returns the waveform and its sampling rate. You don't need to bother much with the audio recording code.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-au1-cC3d5tr"
      },
      "source": [
        "# Helper function to predict the class of audio signal with the model\n",
        "def predict(tensor):\n",
        "    tensor = tensor.to(device)\n",
        "    output = model(tensor.unsqueeze(0))\n",
        "\n",
        "    # index of max value activation\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    return classes[predicted[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hLpLl6w5n9g"
      },
      "source": [
        "from google.colab import output as colab_output\n",
        "from base64 import b64decode\n",
        "from io import BytesIO\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def record(seconds=1):\n",
        "    display(ipd.Javascript(RECORD))\n",
        "    print(f\"Recording started for {seconds} seconds.\")\n",
        "    s = colab_output.eval_js(\"record(%d)\" % (seconds * 1000))\n",
        "    print(\"Recording ended.\")\n",
        "    b = b64decode(s.split(\",\")[1])\n",
        "\n",
        "    fileformat = \"wav\"\n",
        "    filename = f\"_audio.{fileformat}\"\n",
        "    AudioSegment.from_file(BytesIO(b)).export(filename, format=fileformat)\n",
        "    return torchaudio.load(filename)\n",
        "\n",
        "\n",
        "waveform, sample_rate = record()\n",
        "ipd.Audio(waveform.numpy(), rate=sample_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65UH6TtVjAuy"
      },
      "source": [
        "We will now use our model to classify the above recording. For this, first we need to check the length of waveform if it's 16000 and change it if not. Resampling will reduce the length and then we can pad the sample with zeros to 16000 if required. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CKC97aBfvXP"
      },
      "source": [
        "# Model in eval mode\n",
        "model.eval()\n",
        "\n",
        "print('Original sample rate: ', sample_rate)\n",
        "new_sample_rate = 16000\n",
        "# Create a transform and apply to recorded waveform. This will reduce the sample rate close to our desired rate (16000)\n",
        "transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)\n",
        "transformed = transform(waveform)\n",
        "print('New sample rate: ', transformed.shape[1])\n",
        "\n",
        "# Pad with zeros by creating an empty array of 16000 length and place the transformed array into it\n",
        "# To avoid errors with transformed wave sample rate > 16000, we'll consider starting 16000 values (may happen for random audio)\n",
        "padded_waveform = torch.zeros((1,16000))\n",
        "if transformed.shape[-1] < 16000:\n",
        "  padded_waveform[:, :transformed.shape[-1]] = transformed\n",
        "else:\n",
        "  padded_waveform[:, :16000] = transformed[:,:16000]\n",
        "\n",
        "print('Predicted: ', predict(padded_waveform))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HCl26VQksES"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4gowkwreDxX"
      },
      "source": [
        "Q 1: Was any of your recording predicted wrong? If yes, what was the input audio and predicted label and, why do you think model couldn't recognise your word?\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SHm-2R7XWcg"
      },
      "source": [
        "Q 2: Here, we used audio features as 1D input of length 16000. As the features are just numbers, what if we reshape each sample into 2D shape, for eg. 200 x 80 and then use Conv2D instead of Conv1D? Will this work?\n",
        "\n",
        "Answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M43mwhXIWidF"
      },
      "source": [
        "Q 3: What would happen if we remove the Dropout layers from the network? Report the test accuracy and loss. \n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTKPS6ccW3NB"
      },
      "source": [
        "Q 4: What happens if you say combination of two words for eg: 'happybird'? Does the model recognise the first word? Try with the recording. What was the predicted output label?\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXS3j7AfeilI"
      },
      "source": [
        "Q 5: Try training with basic 'StepLR' scheduler and see if the model loses performance or not. What performance did you achieve?\n",
        "\n",
        "Answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMMKlMMtprTK"
      },
      "source": [
        "## References and Additional Resources\n",
        "\n",
        "\n",
        "*   [Torchaudio datasets](https://pytorch.org/audio/stable/datasets.html)\n",
        "*   [Speech commands recognition with torchaudio tutorial - Pytorch](https://pytorch.org/tutorials/intermediate/speech_command_recognition_with_torchaudio.html)\n",
        "*   [Article on Speech Command Classification](https://medium.com/@aminul.huq11/speech-command-classification-using-pytorch-and-torchaudio-c844153fce3b)\n",
        "*   [Dropout regularization](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)\n",
        "*   [CNN for Audio Classification](https://towardsdatascience.com/cnns-for-audio-classification-6244954665ab)\n",
        "*   [Spoken Digits Sound classification](https://medium.com/x8-the-ai-community/audio-classification-using-cnn-coding-example-f9cbd272269e)\n",
        "*   [Sentiment Analysis with CNN](https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7)\n",
        "*   [Text classification using CNNs](https://medium.com/voice-tech-podcast/text-classification-using-cnn-9ade8155dfb9)\n",
        "*   [Adam vs SGD optimizer](https://medium.com/@Biboswan98/optim-adam-vs-optim-sgd-lets-dive-in-8dbf1890fbdc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBe2TO76qPkU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}