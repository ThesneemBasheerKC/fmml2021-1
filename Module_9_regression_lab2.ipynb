{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regression-lab2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fathimath-Rifna-VK/fmml2021/blob/main/Module_9_regression_lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_TjX0v1Upyt"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# interactive visualization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Cp5UaReiof"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbxg8WQcetL2"
      },
      "source": [
        "## Motivation\n",
        "\n",
        "The main goal of machine learning tasks is to \"learn\" something about data we feed to our models. But how does the model actually learn anything? To understand this, we need to first understand what objective functions are.\n",
        "\n",
        "Mathematically, objective functions are a function whose values we desire to minimize/maximize. In the context of machine learning, the objective is commonly referred to as a **loss function**. Loss functions indicate how well the model is doing on the dataset. \n",
        "\n",
        "The value of the objective function is simply called **loss**. Our goal is to eventually find the best model parameters that has the least average loss on the data after training.\n",
        "\n",
        "\n",
        "We are familiar with the idea of **error** from the linear regression lab. RMSE is a common loss function used in machine learning. However, there are many other loss functions employed, depending on what the task is (classification, regression etc.). In this lab, we will take a look at some common loss functions, and understand why they are used where they are.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9lGgMhd9hAX"
      },
      "source": [
        "# 1. Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9qiER5ef8Sh"
      },
      "source": [
        "# initializing random data\n",
        "\n",
        "np.random.seed(0)\n",
        "x = np.linspace(0,100,100)\n",
        "y_actual = np.copy(x)\n",
        "y_pred = x + np.random.randint(-10,10,100)\n",
        "\n",
        "plt.plot(x,y_actual,color=\"red\")\n",
        "plt.scatter(x,y_pred,color=\"blue\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAOKu9cGhf7t"
      },
      "source": [
        "### Mean Squared Error\n",
        "\n",
        "As we have seen before, the formula for MSE is \n",
        "\n",
        "$MSE = \\frac{1}{m} \\sum_{i=1}^n (y-y_p)^2$\n",
        "\n",
        "The image below depicts a visualization of what the squared error is. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=12jmqRrrqknIWKhwRpgRIJAErBjkEcyx9\" width=\"350\" height=\"350\" align=\"middle\"/>\n",
        "\n",
        "MSE is also referred to as L2 loss.\n",
        "\n",
        "Typically, we would want the units of error to be the same as the data, so we use Root Mean Squared Error instead.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0KIs1xP2AbG"
      },
      "source": [
        "mse = np.mean((y_actual-y_pred)**2)\n",
        "print(\"MSE is: {}\".format(mse))\n",
        "print(\"RMSE is: {}\".format(np.sqrt(mse)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Gx5pbfhk3P"
      },
      "source": [
        "### Mean Absolute Error\n",
        "\n",
        "$MAE = \\frac{1}{m} \\sum_{i=1}^n |y-y_p|$\n",
        "\n",
        "MAE is also commonly referred to as L1 loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfqvIN2ThjWO"
      },
      "source": [
        "mae = np.mean(np.abs(y_actual-y_pred))\n",
        "print(\"MAE is: {}\".format(mae))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j7VlwmH4WVU"
      },
      "source": [
        "MSE penalizes the model for making large errors by squaring the difference. However, this also means that MSE cannot handle outliers well, since they would throw the model off.\n",
        "\n",
        "On the other hand, MAE is robust to outliers. But, MAE is non-differentiable, making it difficult to perform mathematical operations on it.\n",
        "\n",
        "Hence, RMSE is most commonly used as a metric for evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buXwWqWjhT_9"
      },
      "source": [
        "# 2. Classification\n",
        "\n",
        "Classification tasks involve some data, along with labels assigned to that data. For example, we may have pictures of cats and dogs, and the corresponding labels assigned to those images. We then want to train a model that can predict a label for some input image belonging to either label class (cat or dog)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiJUcm4vhthC"
      },
      "source": [
        "## The concept of maximum likelihood\n",
        "\n",
        "Ideally, we would want to obtain a model $\\hat{\\theta}$ which maximizes the probability of correctly predicting data. Mathematically, the likelihood is:\n",
        "\n",
        "$ \\prod_{i=1}^{N}  \\: \\hat{\\theta_i}^{\\theta_i}$\n",
        "\n",
        "This is because our actual data ($\\theta_i$) is either 0 or 1 depending on the label of the data, and our predicted data ($\\hat{\\theta_i}$) is a probability. \n",
        "\n",
        "We usually consider the negative log of the likelihood, since log is monotonic and easier to deal with as an optimization problem.\n",
        "\n",
        "Hence, we have\n",
        "\n",
        "$MLE = $ arg min $\\: - \\sum_{i=1}^{N} \\theta_i \\: log \\: \\hat{\\theta_i}$\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "## Entropy and Cross-Entropy\n",
        "\n",
        "For some event $x$, we can obtain **information** based on its probability distribution. For example, for some event that occurs with $p=1$, we gain no information. Now, if we flip a coin and see that we obtained heads, we say that we got 1 bit of information. \n",
        "\n",
        "Thus, the information $I$ of some event $x$ is:\n",
        "\n",
        "$I = -log_2 \\: p(x)$\n",
        "\n",
        "We say that events with a low probability of occuring give high information, and those with a low probability give low information.\n",
        "\n",
        "Now, if we have some random variable $X$, its **entropy** is the expected value of the information obtained.\n",
        "\n",
        "$H(x) = - \\sum_{k} p_k \\: log \\: p_k$\n",
        "\n",
        "Now, suppose we are trying to correctly predict labels of some data. Let P be the true distribution of the labels, and Q be the predicted distribution of labels. \n",
        "\n",
        "Cross-Entropy is then defined as:\n",
        "\n",
        "$H(P,Q) = - \\sum_{x} P(x)\\: log \\: Q(x)$\n",
        "\n",
        "which is basically the entropy between two probability distributions over the same events.\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "## MLE and Cross-Entropy\n",
        "\n",
        "Hence, we have\n",
        "\n",
        "So, in the case of classification, the equation for minimization of the cross-entropy between the actual data and the predicted data would be:\n",
        "\n",
        "arg min $\\: -\\sum_{i=1}^{N} P(x) \\: log \\: Q(x)$\n",
        "\n",
        "$= $ arg min $\\: -\\sum_{i=1}^{N} \\theta_i \\: log \\: \\hat{\\theta_i}$\n",
        "\n",
        "\n",
        "which is exactly the same as what we had obtained for minimizing the negative log likelihood. Hence, the two problems are equivalent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLKuJyHth5-4"
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1 + np.exp(-z))\n",
        "\n",
        "def cross_entropy_loss(y_,y):\n",
        "  if y==1:\n",
        "    return -np.log(y_)\n",
        "  \n",
        "  return -np.log(1-y_)\n",
        "\n",
        "z = np.arange(-10, 10, 0.1)\n",
        "h_z = sigmoid(z)\n",
        "\n",
        "cost_1 = cross_entropy_loss(h_z, 1)\n",
        "cost_0 = cross_entropy_loss(h_z, 0)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "plt.plot(h_z, cost_1, label='y=1')\n",
        "plt.plot(h_z, cost_0, label='y=0')\n",
        "plt.xlabel('predicted probability')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}