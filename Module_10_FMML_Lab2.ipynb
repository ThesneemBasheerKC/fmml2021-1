{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FMML:M10:Lab2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fathimath-Rifna-VK/fmml2021/blob/main/Module_10_FMML_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hwSTRZQoby1"
      },
      "source": [
        "# Introduction to pytorch (contd.)\n",
        "\n",
        "In the previous module, we looked into the basics of `pytorch`. In this module, we'll dig deep into a small example that we have already tackled before: MLP.\n",
        "\n",
        "We'll explore everything in the process of training a simple MLP using `pytorch`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BlFeRURoQ9T"
      },
      "source": [
        "import torch\n",
        "\n",
        "# nn has a lot of things related to training neural networks\n",
        "# optim is a submodule that contains a lot of optimizers, for e.g., SGD, Adam, etc.\n",
        "from torch import nn, optim\n",
        "\n",
        "# DataLoader helps us with flowing data through the model\n",
        "# it lets us control the number of workers loading the data, batch size, etc\n",
        "# we'll look into DataLoaders and Datasets in a future module\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# we'll be using torchvision to download the dataset\n",
        "# this is not needed but just makes it a lot easier\n",
        "# \n",
        "# the transforms are there to help us convert the data\n",
        "# into meaningful network input\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# cool progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# for plotting and viewing images\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8XJQqMqkir"
      },
      "source": [
        "## Setting up the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miSJhWkIpk84"
      },
      "source": [
        "# transforms.Compose lets us apply a bunch of transforms sequentially\n",
        "txs = transforms.Compose([\n",
        "    # convert the input to a torch tensor\n",
        "    transforms.ToTensor(),\n",
        "    # we can add more transforms here\n",
        "    # ...\n",
        "])\n",
        "\n",
        "train_ds = datasets.MNIST(root=\"mnist-data\", train=True, download=True, transform=txs)\n",
        "print(train_ds)\n",
        "test_ds = datasets.MNIST(root=\"mnist-data\", train=False, download=True, transform=txs)\n",
        "print(test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfzOHHWEqmbQ"
      },
      "source": [
        "## Setting up the model\n",
        "\n",
        "For defining a model in `pytorch`, we need to inherit from `nn.Module`\n",
        "\n",
        "After that, we need to define a function `forward` that defines how the data is transformed by the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HbEJCE0qz6A"
      },
      "source": [
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self, input_dim=28*28, output_dim=10, layers=[120, 84]):\n",
        "        # call the constructor of the parent class\n",
        "        # don't forget this otherwise you'll get errors\n",
        "        super().__init__()\n",
        "\n",
        "        # define the layers\n",
        "        # note that this is only defining them\n",
        "        # here, the weights and biases are instantiated\n",
        "        # no training has been done\n",
        "        # this does not define the architecture\n",
        "        self.l1 = nn.Linear(input_dim, layers[0])\n",
        "        self.l3 = nn.Linear(layers[1], output_dim)\n",
        "\n",
        "        # the order in which you define this variables do not matter\n",
        "        self.l2 = nn.Linear(layers[0], layers[1])\n",
        "        self.act = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # here, we define how a set of inputs are to be processed by the model\n",
        "        x = self.l1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.act(x)\n",
        "        x = self.l3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV5rcANur2h_"
      },
      "source": [
        "model = MNISTModel()\n",
        "# again, this does NOT show how the data will get processed\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtWb7wMRsHd8"
      },
      "source": [
        "## Setting up the DataLoader\n",
        "\n",
        "Although it is technically possible to just iterate through the dataset `train_ds` and train the model, it would not be desirable. Using a `DataLoader` offers us a lot of benefits like:\n",
        " - workers: if your dataset needs some processing before going to the model, we can parallelly process data through multiple workers\n",
        " - batch_size: `DataLoader`s allow you to batch your data before sending it to your model, has the option of shuffling it, etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftDr2vDasG4l"
      },
      "source": [
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhoygAAwvdA0"
      },
      "source": [
        "## Setting up the loss and optimizers\n",
        "\n",
        "For this example, we'll use `CrossEntropy` loss, which is used for classification tasks, and, the `Adam` optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VujKYTznw5Zh"
      },
      "source": [
        "# this is not connected to the model as of now\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# the optimizer needs to know about the parameters present in the model\n",
        "# among many other things, we can specify a learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nZPFp_t4Epj"
      },
      "source": [
        "## Setting up device\n",
        "\n",
        "Torch can use the CPUs, GPUs and TPUs for training.\n",
        "TPUs are considered to be the fastest. While colab offers TPUs, we'll use the GPU for training (if available)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFHDLNo74ZxS"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayAeKfN13x3K"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "This is the `L` in `ML`, `DL`\n",
        "\n",
        "We send the data through the model. At each step, the individual tensors have a gradient function attached to them. We calculate the loss and then do backpropagation (`loss.backward()`)\n",
        "\n",
        "Once we do this, the gradients get populated for each parameter.\n",
        "\n",
        "Then, we do `optimizer.step()`. This uses the stored gradients to update the parameters. It keeps track of other things like momentum, etc behind the scenes.\n",
        "\n",
        "NOTE: the gradients need to be zeroed out before each step using `optimizer.zero_grad()`. This makes sure that we only use the gradients calculated in this step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OuONdgB4wB0"
      },
      "source": [
        "def train(model, train_loader, n_epochs, criterion, optimizer, device):\n",
        "    # move the model to the correct device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # keeping track of loss history\n",
        "    # there are much better ways of doing this and much more\n",
        "    # we'll learn about them in a future module (tensorboard, wandb, etc)\n",
        "    loss_history = []\n",
        "\n",
        "    # using tqdm to record progress\n",
        "    with tqdm(range(n_epochs)) as prog_bar:\n",
        "        # loop for n epochs\n",
        "        for epoch in prog_bar:\n",
        "            epoch_loss = 0\n",
        "    \n",
        "            # iterate over the batches\n",
        "            for x, y in train_loader:\n",
        "                # move to correct device\n",
        "                # torch models can run on CPUs GPUs and TPUs\n",
        "                # the data and model need to be on the same device\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "\n",
        "                # we are flattening the 28x28 image to a 784 long vector\n",
        "                x = x.view(x.size(0), -1)\n",
        "\n",
        "                # this zeroes out the gradients of all the parameters\n",
        "                # given to the optimizer.\n",
        "                # basically, clears the gradients from last epoch\n",
        "                optimizer.zero_grad()\n",
        "    \n",
        "                # we send the data through this model\n",
        "                # internally, the `forward` function is called\n",
        "                out = model(x)\n",
        "    \n",
        "                # we calculate the losses...\n",
        "                loss = criterion(out, y)\n",
        "\n",
        "                # ...and then backpropagate them\n",
        "                # this sets the gradients for all the variables\n",
        "                # till this path\n",
        "                # refer to Lab1's Q example\n",
        "                loss.backward()\n",
        "\n",
        "                # now, we call the optimizer\n",
        "                # the optimizer reads these gradients and\n",
        "                # figures out the new weights and biases based\n",
        "                # on (for adam atleast) momentum and other things\n",
        "                optimizer.step()\n",
        "    \n",
        "                # keep track of the loss\n",
        "                epoch_loss += loss\n",
        "            \n",
        "            # we average out the loss\n",
        "            epoch_loss /= len(train_loader)\n",
        "\n",
        "            # update the progress bar\n",
        "            prog_bar.set_description(f\"loss: {epoch_loss}\")\n",
        "\n",
        "            # add this epoch's loss to history\n",
        "            loss_history.append(epoch_loss.detach())\n",
        "    \n",
        "    return loss_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fronsZex4CEq"
      },
      "source": [
        "loss_hist = train(model, train_dl, 10, loss, optimizer, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oGn1iox82Y3"
      },
      "source": [
        "plt.plot(loss_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d5YBbrHvDEM"
      },
      "source": [
        "# References\n",
        " - https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
      ]
    }
  ]
}