{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regression-lab3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fathimath-Rifna-VK/fmml2021/blob/main/Module_9_regression_lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jMitcrlNMvL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize'] = 10, 8\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErY9WV83Nb6I"
      },
      "source": [
        "# Regression Lab 3: Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4iI-n_-viY1"
      },
      "source": [
        "## Motivation behind regularization\n",
        "\n",
        "Most, if not all real world data is noisy, i.e. there are outliers in the data which throw the model off during training. Let's look at a noisy sine wave as an example. We'll try to fit polynomials of various degree to the curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWO75ZhYaIQM"
      },
      "source": [
        "np.random.seed(10)  \n",
        "\n",
        "x = np.array([i*np.pi/180 for i in range(60,300,4)]).reshape((-1,1))\n",
        "y = np.sin(x) + np.random.normal(0,0.15,x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmPJxVOQYPM2"
      },
      "source": [
        "def poly_regression(x,y,deg,show=True):\n",
        "\n",
        "  poly = PolynomialFeatures(degree=deg)\n",
        "\n",
        "  x_ = poly.fit_transform(x)\n",
        "  poly.fit(x_,y)\n",
        "\n",
        "  m1 = LinearRegression()\n",
        "  m1.fit(x_,y)\n",
        "  y_pred = m1.predict(x_)\n",
        "\n",
        "  \n",
        "  if show:\n",
        "    plt.plot(x,y_pred,color=\"red\")\n",
        "    plt.title(\"Visualization of actual data vs predicted outputs (polynomial regression deg={})\".format(deg))\n",
        "    plt.scatter(x,y,color=\"blue\")\n",
        "    plt.legend([\"predicted polynomial\",\"actual data points\"])\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n Coefficients obtained\\n\")\n",
        "    print(m1.coef_)\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0hW2UghnqEc"
      },
      "source": [
        "For now, let's not think about how we obtain these coefficients. We'll look into how they are obtained in a later module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG3Ev4yXaMTw"
      },
      "source": [
        "for deg in range(1,14,3):\n",
        "  y_pred = poly_regression(x,y,deg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oyZPl2fmLVC"
      },
      "source": [
        "This is exactly the same issue as we had seen previously in lab 1! The higher degree polynomials **overfit** the training data, by trying to capture noise in it. Though the model may do well on this set, if we try to fit the learnt curve to another noisy sine wave, it would result in a lower accuracy. Luckily, there's a way around this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ppZxACBmp8q"
      },
      "source": [
        "**Q:** What trend do you observe with respect to the coefficient values as degree grows?\n",
        "\n",
        "\\\n",
        "\n",
        "**A:** The magnitudes of the coefficients go up drastically with degree!\n",
        "\n",
        "\\\n",
        "\n",
        "Now, why does that happen? Intuitively, we can think about what the polynomial is trying to fit here. In the above example, the polynomial is trying to fit to a noisy sine wave. To approximate the large oscillations we see, the coefficients need to have large magnitudes. We can also see that this only happens with higher degree polynomials, since lower degree polynomials cannot even approximate these oscillations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74rKw9wzyELF"
      },
      "source": [
        "## Ridge Regression\n",
        "\n",
        "Ridge regression works by adding a term to the loss function which is proportional to the sum of the squared coefficients of the model.\n",
        "\n",
        "$L_r = \\sum_{i=1}^{N} (y - y_p) ^2 + \\alpha |w|^2$,\n",
        "\n",
        "where $\\sum_{i=1}^{N} (y - y_p) ^2$ is the typical mean-squared error loss and $\\alpha |w|^2$ is the regularization term, $w$ being the coefficient of the term. We can see that it penalizes the squared value of the coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvdklTCyfnYI"
      },
      "source": [
        "def ridge_regression(x,y,deg,alpha):\n",
        "\n",
        "  poly = PolynomialFeatures(degree=deg)\n",
        "  x_ = poly.fit_transform(x)\n",
        "  \n",
        "  r = Ridge(alpha,normalize=True)\n",
        "  r.fit(x_,y)\n",
        "\n",
        "  y_pred_ridge = r.predict(x_)\n",
        "\n",
        "  plt.title(\"Ridge Regressed polynomial deg={}, alpha={}\".format(deg,alpha))\n",
        "  plt.scatter(x,y,color=\"blue\")\n",
        "  plt.plot(x,y_pred_ridge,color=\"red\")\n",
        "  plt.legend([\"actual data\",\"predicted data\"])\n",
        "  plt.show()\n",
        "\n",
        "  print(\"\\n Coefficients obtained\\n\")\n",
        "  print(r.coef_)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyoUQt6Dk1fJ"
      },
      "source": [
        "alpha = 0.1908 #@param {type:\"slider\", min:0, max:1, step:0.0001}\n",
        "\n",
        "for deg in range(1,14,3):\n",
        "  ridge_regression(x,y,deg,alpha=alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgPGs0n1zcZD"
      },
      "source": [
        "## Lasso Regression\n",
        "\n",
        "Lasso regression is similar to ridge regression, except it penalizes the absolute value of the weights.\n",
        "\n",
        "$L_l = \\sum_{i=1}^{N} (y - y_p) ^2 + \\alpha |w|$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzhaPWYIqJY6"
      },
      "source": [
        "def lasso_regression(x,y,deg,alpha):\n",
        "\n",
        "  poly = PolynomialFeatures(degree=deg)\n",
        "  x_ = poly.fit_transform(x)\n",
        "  \n",
        "  l = Lasso(alpha,normalize=True)\n",
        "  l.fit(x_,y)\n",
        "\n",
        "  y_pred_lasso = l.predict(x_)\n",
        "\n",
        "  plt.title(\"Lasso Regressed polynomial deg={}, alpha={}\".format(deg,alpha))\n",
        "  plt.scatter(x,y,color=\"blue\")\n",
        "  plt.plot(x,y_pred_lasso,color=\"red\")\n",
        "  plt.legend([\"actual data\",\"predicted data\"])\n",
        "  plt.show()\n",
        "\n",
        "  print(\"\\n Coefficients obtained\\n\")\n",
        "  print(l.coef_)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNgh_cQ8qUHM"
      },
      "source": [
        "alpha = 0.002 #@param {type:\"slider\", min:0, max:0.01, step:0.001}\n",
        "\n",
        "for deg in range(1,14,3):\n",
        "  lasso_regression(x,y,deg,alpha=alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfCcE1UmrEvk"
      },
      "source": [
        "### Feature selection\n",
        "\n",
        "Sometimes, data may contain features that are not relevant to the model; they do not contribute to the model's predictions. Naturally, we would want to get rid of such features in our data. **Feature selection** is a process by which we understand which features are actually relevant to the model and remove the unrelated features from our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwydhHOCqgVJ"
      },
      "source": [
        "The advantage of lasso regularization over ridge regularization is that it does a better job of feature selection. In ridge regression, the coefficient can approach zero, but never truly reaches zero. Hence, the model cannot perform feature selection.\n",
        "\n",
        "In lasso regression, since we are penalizing the absolute value of the coefficient, the feature can have a zero coefficient, which is the same as feature selection. In the above example, we can clearly see the feature selection process in action, with higher degree polynomials have zero-valued coefficients for some of the terms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uJlAzuTunuO"
      },
      "source": [
        "## Applying regularization to the crime dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNQJBcEAutAu"
      },
      "source": [
        "src = 'http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data'\n",
        "crime = pd.read_csv(src, header=None, na_values=['?'])\n",
        "\n",
        "crime.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcHCxEzFvAo4"
      },
      "source": [
        "crime.drop([0, 1, 2, 3, 4], axis=1, inplace=True)\n",
        "crime.dropna(inplace=True)\n",
        "\n",
        "# checking for any null values\n",
        "crime.isnull().any().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "equG48iavjkn"
      },
      "source": [
        "def lin_regression(x,y,show=True):\n",
        "\n",
        "  # learning the coefficient and intercept\n",
        "\n",
        "  x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n",
        "\n",
        "  m1 = LinearRegression()\n",
        "  m1.fit(x_train,y_train)\n",
        "\n",
        "  y_pred = m1.predict(x_test)\n",
        "  metrics.mean_squared_error(y_pred,y_test)\n",
        "\n",
        "  # plotting data and predictions\n",
        "\n",
        "  if show:\n",
        "\n",
        "    print(\"The coefficients are: \\n {} \\n\\n The intercept is: {}\\n\".format(m1.coef_,m1.intercept_))\n",
        "    print(\"The RMSE for linear regression is: {}\".format(math.sqrt(metrics.mean_squared_error(y_pred,y_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI7DcptkvRXU"
      },
      "source": [
        "x_crime = crime.drop(127,axis=1)\n",
        "y_crime = crime[127]\n",
        "\n",
        "lin_regression(x_crime,y_crime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiGqKKLTyq1s"
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x_crime,y_crime,test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex34CY-lwKd2"
      },
      "source": [
        "r = Ridge(alpha=0.2,normalize=True)\n",
        "r.fit(x_train,y_train)\n",
        "y_pred_ridge = r.predict(x_test)\n",
        "\n",
        "print(\"The coefficients are: \\n {}\".format(r.coef_))\n",
        "print(\"The RMSE for ridge regression is: {}\".format(math.sqrt(metrics.mean_squared_error(y_pred_ridge,y_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfp6mDKczJCM"
      },
      "source": [
        "l = Lasso(alpha=0.002,normalize=True)\n",
        "l.fit(x_train,y_train)\n",
        "y_pred_lasso = l.predict(x_test)\n",
        "\n",
        "print(\"The coefficients are: \\n {}\".format(l.coef_))\n",
        "print(\"The RMSE for lasso regression is: {}\".format(math.sqrt(metrics.mean_squared_error(y_pred_lasso,y_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5m6ZY-v0JTK"
      },
      "source": [
        "## References\n",
        "\n",
        "1. https://harish-reddy.medium.com/regularization-in-python-699cfbad8622"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOtXOnOQ0Qyv"
      },
      "source": [
        "## Further Explorations\n",
        "\n",
        "1. https://en.wikipedia.org/wiki/Elastic_net_regularization"
      ]
    }
  ]
}