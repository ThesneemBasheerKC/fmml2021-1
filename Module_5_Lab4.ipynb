{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fathimath-Rifna-VK/fmml2021/blob/main/Module_5_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MmNuWPfqkjH"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKyXjRIQq0f8"
      },
      "source": [
        "N_samples = 400\n",
        "x_1 = np.random.uniform(low= - 4.0, high = 4.0, size=N_samples)\n",
        "x_2 = np.random.uniform(low = -4.0, high = 4.0, size=N_samples)\n",
        "noise = np.random.normal(0, 0.01, N_samples)\n",
        "y = 2 * x_1 + 3 * x_2 + noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kryBfxNxtEW2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRajYKOSq4tc"
      },
      "source": [
        "a = np.linspace(-10, 10, 200)\n",
        "b = np.linspace(-10, 10, 200)\n",
        "# a, b are the parameter values\n",
        "\n",
        "X, Y = np.meshgrid(a, b)\n",
        "J = np.ones(shape = (len(a), len(b)))\n",
        "\n",
        "# compute J for all pairs of a, b\n",
        "for i in range(len(a)):\n",
        "    for j in range(len(b)):\n",
        "        J[i][j] = np.linalg.norm((2 * x_1 + 3 * x_2) - (a[i] * x_1 + b[j] * x_2)) / N_samples\n",
        "\n",
        "# creating the 3D Plot\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "sub = fig.add_subplot(111, projection='3d')\n",
        "sub.set_title('Relationship between J and (a, b)')\n",
        "sub.set_xlabel('a')\n",
        "sub.set_ylabel('b')\n",
        "sub.set_zlabel('J')\n",
        "sub.plot_surface(X, Y, J, cmap = cm.coolwarm)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5kpOrJ9q8Lf"
      },
      "source": [
        "# calculating the optimal learning rate:\n",
        "\n",
        "Hessian = 2 * np.asarray([[np.dot(x_1, x_1), np.dot(x_1, x_2)], [np.dot(x_1, x_2), np.dot(x_2, x_2)]])\n",
        "eigvals, _ = np.linalg.eig(Hessian)\n",
        "eta_opt = 1 / max(eigvals)\n",
        "print(\"Optimal Learning Rate:\", str(eta_opt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Noqr-gLkq-fg"
      },
      "source": [
        "def loss(a, b):\n",
        "    return np.linalg.norm((a * x_1 + b * x_2) - y) / N_samples\n",
        "\n",
        "def gradient_descent(eta_multiplier):\n",
        "    global Hessian\n",
        "    a = b = 0\n",
        "    errors = [loss(a, b)]\n",
        "    seq = [(0, 0)]\n",
        "    MAX_ITER = 100\n",
        "    iteration = 0\n",
        "\n",
        "    while errors[-1] >= 1e-3 and iteration < MAX_ITER:\n",
        "        # using the accurate version of optimal learning rate\n",
        "        del_J_del_a = 2 * a * np.dot(x_1, x_1) + 2 * b * np.dot(x_1, x_2) - 2 * np.dot(y, x_1)\n",
        "        del_J_del_b = 2 * a * np.dot(x_1, x_2) + 2 * b * np.dot(x_2, x_2) - 2 * np.dot(y, x_2)\n",
        "        del_J = np.asarray([del_J_del_a, del_J_del_b])\n",
        "        if np.matmul(np.matmul(del_J.T, Hessian), del_J) == 0:\n",
        "            # indicates optimum has been reached\n",
        "            break\n",
        "            \n",
        "        eta_opt = np.dot(del_J, del_J) / np.matmul(np.matmul(del_J.T, Hessian), del_J)\n",
        "        \n",
        "        eta = eta_opt * eta_multiplier\n",
        "\n",
        "        # update step\n",
        "        a = a - eta * del_J_del_a\n",
        "        b = b - eta * del_J_del_b\n",
        "        seq.append((a, b))\n",
        "        errors.append(loss(a, b))\n",
        "        iteration += 1\n",
        "    \n",
        "    return errors, seq, a, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSpXXNo7q_Gk"
      },
      "source": [
        "def make_plot(seq, errors):\n",
        "    make_error_plot(errors)\n",
        "    make_convergence_plot(seq)\n",
        "\n",
        "def make_error_plot(errors):\n",
        "    fig = plt.figure(figsize=(7, 7))\n",
        "    p = fig.add_subplot('111')\n",
        "    p.set_title('Error v/s Epoch')\n",
        "    p.set_xlabel('Epoch Number')\n",
        "    p.set_ylabel('Error')\n",
        "    p.plot(list(range(1, len(errors))), errors[1:])\n",
        "    plt.show()\n",
        "\n",
        "def make_convergence_plot(seq, problem = 1, xlabel = 'a', ylabel = 'b'):\n",
        "    # problem can be 1, 2, 3\n",
        "    fig = plt.figure(figsize=(7, 7))\n",
        "    q = fig.add_subplot('111')\n",
        "\n",
        "    a_min = min([s[0] for s in seq])\n",
        "    a_max = max([s[0] for s in seq])\n",
        "\n",
        "    b_min = min([s[1] for s in seq])\n",
        "    b_max = max([s[1] for s in seq])\n",
        "\n",
        "    eps = 0.95\n",
        "    if xlabel == 'a':\n",
        "        eps = 0.2\n",
        "\n",
        "    if problem == 1:\n",
        "        a = np.linspace(a_min - eps, a_max + eps, 500)\n",
        "        b = np.linspace(b_min - eps, b_max + eps, 500)\n",
        "    \n",
        "    else:\n",
        "        a = np.linspace(-1, 1, 500)\n",
        "        b = np.linspace(-1, 1, 500)\n",
        "\n",
        "    X, Y = np.meshgrid(a, b)\n",
        "    J = np.ones(shape = (len(a), len(b)))\n",
        "\n",
        "    # compute J for all pairs of a, b\n",
        "    for i in range(len(a)):\n",
        "        for j in range(len(b)):\n",
        "            if problem == 1:\n",
        "                J[i][j] = np.linalg.norm((2 * x_1 + 3 * x_2) - (a[i] * x_1 + b[j] * x_2))\n",
        "            elif problem == 2:\n",
        "                J[i][j] = a[i] * a[i] + 100 * np.power((b[j] - a[i] * a[i]), 2) \n",
        "            elif problem == 3:\n",
        "                J[i][j] = 50 / 9 * np.power((a[i] * a[i] + b[j] * b[j]), 3) - 209 / 18 * np.power((a[i] * a[i] + b[j] * b[j]), 2) + 59 / 9 * (a[i] * a[i] + b[j] * b[j])\n",
        "    \n",
        "    q.set_title('Convergence Path')\n",
        "    q.set_xlabel(xlabel)\n",
        "    q.set_ylabel(ylabel)\n",
        "    q.contour(X, Y, J, cmap = cm.coolwarm)\n",
        "\n",
        "    # plotting the arrow-ed path\n",
        "    aspace = 0.1 # scaling factor\n",
        "\n",
        "    # r is the distance spanned between pairs of points\n",
        "    r = [0]\n",
        "    for i in range(1,len(seq)):\n",
        "        dx = seq[i][0]-seq[i-1][0]\n",
        "        dy = seq[i][1]-seq[i-1][1]\n",
        "        val = np.sqrt(dx ** 2 + dy ** 2)\n",
        "        r.append(val)\n",
        "    r = np.asarray(r)\n",
        "\n",
        "    # r_sum is the cumulative sum of r\n",
        "    r_sum = []\n",
        "    for i in range(len(r)):\n",
        "        r_sum.append(r[0: i].sum())\n",
        "    r_sum.append(r.sum())\n",
        "\n",
        "    arrow_data = [] # holds tuples of (x, y, theta) for each arrow\n",
        "    arrow_pos = 0 # current point on walk along data\n",
        "    r_count = 1\n",
        "\n",
        "    while arrow_pos < r.sum():\n",
        "        x1, x2 = seq[r_count-1][0], seq[r_count][0]\n",
        "        y1, y2 = seq[r_count-1][1], seq[r_count][1]\n",
        "        da = arrow_pos - r_sum[r_count] \n",
        "        theta = np.arctan2((x2 - x1), (y2 - y1))\n",
        "        ax = np.sin(theta) * da + x1\n",
        "        ay = np.cos(theta) * da + y1\n",
        "\n",
        "        arrow_data.append((ax, ay, theta))\n",
        "        arrow_pos += aspace\n",
        "\n",
        "        while arrow_pos > r_sum[r_count + 1]: \n",
        "            r_count += 1\n",
        "            if arrow_pos > r_sum[-1]:\n",
        "                break\n",
        "\n",
        "    for ax, ay, theta in arrow_data:\n",
        "        q.arrow(ax, ay, np.sin(theta) * aspace / 10, np.cos(theta) * aspace / 10, head_width = aspace / 2.5 , head_length = aspace / 1.5 , color = 'darkgreen')\n",
        "\n",
        "    q.plot([s[0] for s in seq], [s[1] for s in seq], color = 'green')\n",
        "    \n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkd0njxUuqoO"
      },
      "source": [
        "# eta = (0.45) * eta_opt\n",
        "e, s, a_opt, b_opt = gradient_descent((0.45))\n",
        "make_plot(s, e)\n",
        "# output:\n",
        "print(\"Optimal Values for: \")\n",
        "print(\"a : \" + str(a_opt))\n",
        "print(\"b : \" + str(b_opt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X2w5U0eraBT"
      },
      "source": [
        "# eta = eta_opt\n",
        "e, s, a_opt, b_opt = gradient_descent(1)\n",
        "make_plot(s, e)\n",
        "# output:\n",
        "print(\"Optimal Values for: \")\n",
        "print(\"a : \" + str(a_opt))\n",
        "print(\"b : \" + str(b_opt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mt2KgWeu0BN"
      },
      "source": [
        "# eta = (1.5) * eta_opt\n",
        "e, s, a_opt, b_opt = gradient_descent(1.5)\n",
        "make_plot(s, e)\n",
        "# output:\n",
        "print(\"Optimal Values for: \")\n",
        "print(\"a : \" + str(a_opt))\n",
        "print(\"b : \" + str(b_opt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lKoIhqArflB"
      },
      "source": [
        "def check_convergence(seq):\n",
        "    threshold = 1e-8\n",
        "    if len(seq) > 1 and np.abs(seq[-1][0] - seq[-2][0]) < threshold and np.abs(seq[-1][1] - seq[-2][1]) < threshold:\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEHUPrNir6Io"
      },
      "source": [
        "def plain_gradient_descent(f, grad_x, grad_y):\n",
        "    # f is the function to be minimized\n",
        "    # grad_x and grad_y are functions which accept (x, y)\n",
        "    # and return partial derivatives of f w.r.t x and y respectively\n",
        "    \n",
        "    x = np.random.random()\n",
        "    y = np.random.random()\n",
        "    # initialized values\n",
        "\n",
        "    seq = [(x, y)] # sequence of x, y\n",
        "    MAX_ITER = 1e5\n",
        "    iteration = 0\n",
        "    eta = 1e-3\n",
        "\n",
        "    while iteration < MAX_ITER and check_convergence(seq) == False:\n",
        "        # update step\n",
        "        del_x = grad_x(x, y)\n",
        "        del_y = grad_y(x, y)\n",
        "        x = x - eta * del_x\n",
        "        y = y - eta * del_y\n",
        "        \n",
        "        seq.append((x, y))\n",
        "        iteration += 1\n",
        "    \n",
        "    print(\"Number of Iterations: \" + str(iteration))\n",
        "    print(\"f* = \" + str(f(x, y)))\n",
        "    print(\"(x*, y*) = \" + \"(\" + str(seq[-1][0]) + \", \" + str(seq[-1][1]) + \")\")\n",
        "    return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-xPgijArgGS"
      },
      "source": [
        "def nesterov_gradient_descent(f, grad_x, grad_y):\n",
        "    # f is the function to be minimized\n",
        "    # grad_x and grad_y are functions which accept (x, y)\n",
        "    # and return partial derivatives of f w.r.t x and y respectively\n",
        "    \n",
        "    x = np.random.random()\n",
        "    y = np.random.random()\n",
        "    V_x = [0]\n",
        "    V_y = [0]\n",
        "    # initialized values\n",
        "\n",
        "    seq = [(x, y)] # sequence of x, y\n",
        "    MAX_ITER = 1e5\n",
        "    iteration = 0\n",
        "    alpha = 1e-4\n",
        "    beta = 0.9\n",
        "\n",
        "    while iteration < MAX_ITER and check_convergence(seq) == False:\n",
        "        # update step\n",
        "        x_star = x - alpha * V_x[-1]\n",
        "        y_star = y - alpha * V_y[-1]\n",
        "        \n",
        "        del_x = grad_x(x_star, y_star)\n",
        "        del_y = grad_y(x_star, y_star)\n",
        "\n",
        "        V_x.append(beta * V_x[-1] + (1 - beta) * del_x)\n",
        "        V_y.append(beta * V_y[-1] + (1 - beta) * del_y)\n",
        "                   \n",
        "        x = x - alpha * V_x[-1]\n",
        "        y = y - alpha * V_y[-1]\n",
        "        \n",
        "        seq.append((x, y))\n",
        "        iteration += 1\n",
        "\n",
        "    print(\"Number of Iterations: \" + str(iteration))\n",
        "    print(\"f* = \" + str(f(x, y)))\n",
        "    print(\"(x*, y*) = \" + \"(\" + str(seq[-1][0]) + \", \" + str(seq[-1][1]) + \")\")\n",
        "    return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uBxohhqrjRw"
      },
      "source": [
        "seq_plain = plain_gradient_descent(lambda x, y: x ** 2 + 100 * ((y - x ** 2) ** 2),lambda x, y: (2 * x * (1 - 200 * (y - x ** 2))),  lambda x, y : (200 * (y - x ** 2)))\n",
        "make_convergence_plot(seq_plain, 2, 'x', 'y')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyHVCBnZrjuh"
      },
      "source": [
        "seq_nesterov = nesterov_gradient_descent(lambda x, y: x ** 2 + 100 * ((y - x ** 2) ** 2), lambda x, y: (2 * x * (1 - 200 * (y - x ** 2))),  lambda x, y : (200 * (y - x ** 2)))\n",
        "make_convergence_plot(seq_nesterov, 2, 'x', ylabel = 'y')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT8gfzMeszxI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}